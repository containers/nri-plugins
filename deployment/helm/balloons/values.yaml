# Default values for nri-plugins.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
---
image:
  name: ghcr.io/containers/nri-plugins/nri-resource-policy-balloons
  # tag, if defined will use the given image tag, otherwise Chart.AppVersion will be used
  #tag: unstable
  pullPolicy: Always

config:
  reservedResources:
    cpu: 750m
  idleCPUClass: normal
  allocatorTopologyBalancing: true
  balloonTypes:
  - name: default
    namespaces:
      - default
    minCPUs: 1
    minBalloons: 0
    allocatorPriority: normal
    shareIdleCPUsInSame: system
  reservedPoolNamespaces:
    - kube-system
  log:
    source: true
    klog:
      skip_headers: true
  instrumentation:
    reportPeriod: 60s
    samplingRatePerMillion: 0

# configGroupLabel: config.nri/group

# Extra environment variables to inject.
# extraEnv:
#   VAR1: VAL1
#   VAR2: VAL2

plugin-test:
  enableAPIs: false

hostPort: 8891

resources:
  cpu: 500m
  memory: 512Mi

nri:
  plugin:
    index: 90
  runtime:
    patchConfig: false
#   config:
#     pluginRegistrationTimeout: 5s
#     pluginRequestTimeout: 2s

initContainerImage:
  name: ghcr.io/containers/nri-plugins/nri-config-manager
  # If not defined Chart.AppVersion will be used
  #tag: unstable
  pullPolicy: Always

tolerations: []
#
# Example:
#
# tolerations:
# - key: "node-role.kubernetes.io/control-plane"
#   operator: "Exists"
#   effect: "NoSchedule"

affinity: []
#
# Example:
#
# affinity:
#   nodeAffinity:
#     requiredDuringSchedulingIgnoredDuringExecution:
#       nodeSelectorTerms:
#       - matchExpressions:
#         - key: topology.kubernetes.io/disk
#           operator: In
#           values:
#           - ssd

nodeSelector: []
#
# Example:
#
# nodeSelector:
#  kubernetes.io/disk: "ssd"

# NRI plugins should be considered as part of the container runtime.
# By default we make them part of the system-node-critical priority
# class. This should mitigate the potential risk of a plugin getting
# evicted under heavy system load. It should also ensure that during
# autoscaling enough new nodes are brought up to leave room for the
# plugin on each new node.
podPriorityClassNodeCritical: true
